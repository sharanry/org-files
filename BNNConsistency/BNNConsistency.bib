@ARTICLE{NNnonsmooth,
       author = {{Imaizumi}, Masaaki and {Fukumizu}, Kenji},
        title = "{Deep Neural Networks Learn Non-Smooth Functions Effectively}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning},
         year = "2018",
        month = "Feb",
          eid = {arXiv:1802.04474},
        pages = {arXiv:1802.04474},
archivePrefix = {arXiv},
       eprint = {1802.04474},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180204474I},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{cannings2018classification,
  title={Classification with imperfect training labels},
  author={Cannings, Timothy I and Fan, Yingying and Samworth, Richard J},
  journal={arXiv preprint arXiv:1805.11505},
  year={2018}
}

@incollection{natarajan2013learning,
title = {Learning with Noisy Labels},
author = {Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {1196--1204},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5073-learning-with-noisy-labels.pdf}
}


@inproceedings{li2017learning,
  title={Learning from noisy labels with distillation},
  author={Li, Yuncheng and Yang, Jianchao and Song, Yale and Cao, Liangliang and Luo, Jiebo and Li, Li-Jia},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1910--1918},
  year={2017}
}

@inproceedings{patrini2017making,
  title={Making deep neural networks robust to label noise: A loss correction approach},
  author={Patrini, Giorgio and Rozza, Alessandro and Krishna Menon, Aditya and Nock, Richard and Qu, Lizhen},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1944--1952},
  year={2017}
}

@article{rolnick2017deep,
  title={Deep learning is robust to massive label noise},
  author={Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},
  journal={arXiv preprint arXiv:1705.10694},
  year={2017}
}

@article{VC-CNN,
  author    = {Saikat Basu and
               Manohar Karki and
               Robert DiBiano and
               Supratik Mukhopadhyay and
               Sangram Ganguly and
               Ramakrishna R. Nemani and
               Shreekant Gayaka},
  title     = {A Theoretical Analysis of Deep Neural Networks for Texture Classification},
  journal   = {CoRR},
  volume    = {abs/1605.02699},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.02699},
  archivePrefix = {arXiv},
  eprint    = {1605.02699},
  timestamp = {Mon, 13 Aug 2018 16:48:27 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BasuKDMGNG16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{NNStrongConsistency,
author={A. {Farago} and G. {Lugosi}},
journal={IEEE Transactions on Information Theory},
title={Strong universal consistency of neural network classifiers},
year={1993},
volume={39},
number={4},
pages={1146-1151},
keywords={error statistics;learning (artificial intelligence);neural nets;nonparametric statistics;pattern recognition;convergence rate;universal consistency;neural network classifiers;statistical pattern recognition;error probability;Bayes-risk;training data;observation vector;one-layered neural network;smoothness conditions;training algorithm;polynomial time;space dimension;Neural networks;Error probability;Training data;Random variables;Kernel;Feedforward neural networks;Pattern recognition;Convergence;Polynomials;Classification algorithms},
doi={10.1109/18.243433},
ISSN={0018-9448},
month={July},}

@inproceedings{vladimirova2019understanding,
  title={Understanding priors in Bayesian neural networks at the unit level},
  author={Vladimirova, Mariia and Verbeek, Jakob and Mesejo, Pablo and Arbel, Julyan},
  booktitle={International Conference on Machine Learning},
  pages={6458--6467},
  year={2019}
}

@article{lee2000consistency,
  title={Consistency of posterior distributions for neural networks},
  author={Lee, Herbert KH},
  journal={Neural Networks},
  volume={13},
  number={6},
  pages={629--642},
  year={2000},
  publisher={Elsevier}
}

@ARTICLE{CriteriaPosteriorConsistence,
       author = {{Kleijn}, B.~J.~K. and {Zhao}, Y.~Y.},
        title = "{Criteria for posterior consistency}",
      journal = {arXiv e-prints},
     keywords = {Mathematics - Statistics Theory, 62G05, 62G07, 62G08, 62G20},
         year = "2013",
        month = "Aug",
          eid = {arXiv:1308.1263},
        pages = {arXiv:1308.1263},
archivePrefix = {arXiv},
       eprint = {1308.1263},
 primaryClass = {math.ST},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013arXiv1308.1263K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{rockova2018posterior,
  title={Posterior concentration for sparse deep learning},
  author={Rockova, Veronika and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={930--941},
  year={2018}
}

@ARTICLE{adaptiveSparseness,
author={M. A. T. {Figueiredo}},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Adaptive sparseness for supervised learning},
year={2003},
volume={25},
number={9},
pages={1150-1159},
keywords={learning by example;generalisation (artificial intelligence);Bayes methods;maximum likelihood estimation;inference mechanisms;statistical analysis;adaptive sparseness;supervised learning;functional mapping;training examples;generalization;Bayesian approaches;sparse solutions;hierarchical-Bayes interpretation;Laplacian prior;noninformative hyperprior;expectation-maximization algorithm;experiments;benchmark data sets;hyperparameters;Supervised learning;Bayesian methods;Training data;Support vector machines;Support vector machine classification;Automatic control;Laplace equations;Kernel;Neural networks;Feedforward neural networks},
doi={10.1109/TPAMI.2003.1227989},
ISSN={0162-8828},
month={Sep.},}
