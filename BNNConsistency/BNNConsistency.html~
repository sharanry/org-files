<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Studying Uncertainity in Bayesian Neural Networks</title>
<!-- 2019-07-17 Wed 11:04 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="Sharan Yalburgi, Julyan Arbel" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "left",
        displayIndent: "5em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Studying Uncertainity in Bayesian Neural Networks</h1>



<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Abstract</h2>
<div class="outline-text-2" id="text-1">
<p>
Availability of uncertainity measure is the one of the major reasons why Bayesian Neural Networks are topics of interest. It is therefore essential to understand the behavior of these uncertainity measures with varying prior conditions, amount of data and modelling tasks.
</p>
</div>
</div>


<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Introduction</h2>
<div class="outline-text-2" id="text-2">
<p>
Contributions of this paper is as follows:
</p>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Previous Work</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> Study of Bayesian Neural Networks with Non-Smooth Functions \cite{NNnonsmooth}</h3>
</div>

<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> Consistency of Posterior Distributions for Neural Networks \cite{lee2000consistency}</h3>
</div>

<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="section-number-3">3.3</span> Classification kwith imperfect training labels \cite{cannings2018classification}</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li>Study effect of imperfect training labels on performance of classification methods.
</li>

<li>Bound the excess risk of an arbitrary classifier trained with imperfect labels in terms of its excess risk for predicting a noisy labe
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-3-4" class="outline-3">
<h3 id="sec-3-4"><span class="section-number-3">3.4</span> A Theoretical Analysis of Deep Neural Networks for Texture Classification \cite{VC-CNN}</h3>
<div class="outline-text-3" id="text-3-4">
<ul class="org-ul">
<li>Derives the upper bounds on the VC dimension of Convolutional Neural Network as well as Dropout and Dropconnect networks and the relation between excess error rate of Dropout and Dropconnect networks.
</li>

<li>Introduces concept  of Intrinsic  Dimension to  show  that  texture  datasets  have  a higher dimensionality than color/shape based data.
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-3-5" class="outline-3">
<h3 id="sec-3-5"><span class="section-number-3">3.5</span> Understanding Priors in Bayesian Neural Networks at the Unit Level - \cite{vladimirova2019understanding}</h3>
<div class="outline-text-3" id="text-3-5">
<ul class="org-ul">
<li>Studies the "induced" prior distributions at a unit level. 
<ul class="org-ul">
<li>I layer units are Gaussian
</li>
<li>II layer units are sub-exponential
</li>
<li>Units in deeper layers are characterized by sub-Weibull distributions.
</li>
</ul>
</li>
<li>This paper is devoted to the investigation of hidden units prior distributions in Bayesian neural networks under the assumption of independent Gaussian weights.
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-3-6" class="outline-3">
<h3 id="sec-3-6"><span class="section-number-3">3.6</span> Consistency of posterior distributions for neural networks - \cite{lee2000consistency}</h3>
<div class="outline-text-3" id="text-3-6">
<p>
Considers only feedforward neural networks with a single hidden layer of units with logistic activation functions(not done for popularly used ReLU activations) and linear output unit(no Sigmoid at the end).
Shows that posterior probability of feedforward neural networks is \textbf{"asymptotically consistent"}. \( \hat{g_n} \) is asymptotically consistent for true regression function \( g_0 \) if
</p>
\begin{equation}
    \int (\hat{g_n}(x) - g_0(x))^2f_0(x)dx \xrightarrow{p} 0
\end{equation}
<p>
\( p \) here means that the probability, 
</p>

<p>
\( P(|\int(\hat{g_n}(x) - g_0(x))^2f_0(x)dx| > \epsilon) \rightarrow 0 \) as \( n \rightarrow \infty\).
\\
\textbf{Asymptotically consistence in terms of Hellinger neighbourhood:} If \( (X_i, Y_i) ~ f_0 \), the posterior is asymptotically consistence for \(f_0\)(true joint distribution over X \&amp; Y) over Hellinger neighbourhood if for every \(epsilon>0\)
</p>
\begin{equation}
    P({f: D_H(f,f_0) <= \epsilon}|(X_i, Y_i) \forall i) \xrightarrow{p} 1
\end{equation}
<p>
where Hellinger distance $$D_H$$ is defined as 
</p>
\begin{equation*}
    D_H(f, f_0) = \sqrt{\int\int(\sqrt{f(x, y)}-\sqrt{f_0(x, y)})^2 dx dy}
\end{equation*}
<p>
\textbf{Another approach to consistency}: Posterior probability of every neighbourhood of true function tends to 1, i.e, \(P(A_\epsilon|Y) \longrightarrow 1\) as \(n\rightarrow{}1\).
\\\\
\textbf{Sieve Approach:} The number of hidden nodes grows with the sample size so that asymptotically they use arbitrarily large number of nodes.
\\\\
\textbf{Parameter Approach:} Number of hidden nodes taken as a parameter(not sure if as a hyperparameter) and show that joint posterior is also consistent.
Extends earlier results on "universal approximation properties of neural networks to the Bayesian setting".
\\\\
Shows mathematically that using a neural network to estimate and \textbf{continuous or square integrable} function will be consistent with probability tending to one given enough data. This is when the number of nodes grew with amount of data in a controlled way but \(k(n) \xrightarrow{} \infty\) as \(n \xrightarrow{} \infty\) where \(k\) is the number of nodes in the network or when the number of nodes is a parameter which can be estimated from the data.    
</p>
</div>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> Theory</h2>
</div>


<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> Experiments</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-sec-5-1" class="outline-3">
<h3 id="sec-5-1"><span class="section-number-3">5.1</span> Results from \cite{cannings2018classification}</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>SVMs and Knns are robust/consistent to corrupted/imperfect data.
</li>
<li>Whereas, LDAs are not unless prior probabilities of each class are equal. 
</li>
</ul>



\begin{figure*}[ht]
\begin{tabular}{cc}

\subfloat[]{\includegraphics[width = 3.1in]{knn_100runs.png}} &
\subfloat[]{\includegraphics[width = 3.1in]{svc_100runs.png}} \\
\subfloat[]{\includegraphics[width = 3.1in]{lda_100runs.png}} &
\subfloat[]{\includegraphics[width = 3.1in]{perceptron_100runs.png}}

\end{tabular}
\caption{Reproduced results from \cite{cannings2018classification} with data generated from multivariate Gaussian \& Perceptron Findings}
\end{figure*}
</div>
</div>

<div id="outline-container-sec-5-2" class="outline-3">
<h3 id="sec-5-2"><span class="section-number-3">5.2</span> Consistency in Perceptron and Deep Neural Networks</h3>
<div class="outline-text-3" id="text-5-2">
\begin{figure*}[!htp]
  % Maximum length
\subfloat[Prediction contour of Perceptron without Sigmoid activation]{\label{percep_w_sig}\includegraphics[height=3cm]{simple_models/percep_without_sigmoid.png}}\hfill
\subfloat[Prediction contour of Perceptron without Sigmoid activation]{\label{percep_sig}\includegraphics[height=3cm]{simple_models/percep_with_sigmoid.png}}\hfill
\subfloat[Prediction contour of Single Hidden Layer Neural Network without Sigmoid activation]{\label{simple_nn}\includegraphics[height=3cm]{simple_models/simple_neural_net.png}}
   \caption{Analysis of Consistency of Perceptron with noisy data}
\end{figure*}


\begin{figure*}[!htp]
  % Maximum length
  \subfloat[Training losses of the different instances of the simple neural network]{\label{fig2:a}\includegraphics[width=1.00\linewidth]{NN_consistency/losses.jpg}}\\
  \subfloat[Average error rates of the samples trained single hidden layer neural networks.]{\label{fig2:b}\includegraphics[width=0.33\linewidth]{NN_consistency/errors.jpg}}\hfill
  \subfloat[Prediction contour of trained neural network with original data.(without noise)]{\label{fig2:c}\includegraphics[width=0.33\linewidth]{NN_consistency/orig_simple_neural_network_model2_contour.jpg}}\hfill
  \subfloat[Prediction contour of trained neural network with original data.(without noise)]{\label{fig2:d}\includegraphics[width=0.33\linewidth]{NN_consistency/flipped_simple_neural_network_model2_contour.jpg}}
   \caption{Analysis of Consistency of Neural Networks with noisy data}
\end{figure*}
</div>
</div>



<div id="outline-container-sec-5-3" class="outline-3">
<h3 id="sec-5-3"><span class="section-number-3">5.3</span> Analysis of binary two-moons dataset classification using Bayesian Neural Networks(PyMC3)</h3>
<div class="outline-text-3" id="text-5-3">
\begin{figure*}[!htp]
    \subfloat[Toy Two Moon Dataset]{\label{2moondataset}\includegraphics[width=0.50\linewidth]{bnn_classification_two_moon/dataset.png}}\hfill
    \subfloat[Prediction of Test Set by the Bayesian-NN  ]{\label{2moonpredicted}\includegraphics[width=0.50\linewidth]{bnn_classification_two_moon/predicted.png}}\\
    \subfloat[Prediction Contour]{\label{2moon_pred_contour}\includegraphics[width=0.50\linewidth]{bnn_classification_two_moon/prediction_contour.png}}\hfill
    \subfloat[Uncertainty Contour]{\label{2moon_uncer_contour}\includegraphics[width=0.50\linewidth]{bnn_classification_two_moon/uncertainity_contour.png}}\\
    \caption{Bayesian Neural Network trained to classify Two-Moon Dataset}
\end{figure*}
</div>
</div>


<div id="outline-container-sec-5-4" class="outline-3">
<h3 id="sec-5-4"><span class="section-number-3">5.4</span> Analysis of Bayesian Regression</h3>
<div class="outline-text-3" id="text-5-4">
</div><div id="outline-container-sec-5-4-1" class="outline-4">
<h4 id="sec-5-4-1">Analysis of continuous regression using Bayesian Neural Networks(numpyro)</h4>
<div class="outline-text-4" id="text-5-4-1">
\begin{figure*}[!htp]
    \subfloat[sigma: 0.2 ]{\includegraphics[width=0.20\linewidth]{continuous_regression/cont_reg_02.png}}\hfill
    \subfloat[sigma: 0.5 ]{\includegraphics[width=0.20\linewidth]{continuous_regression/cont_reg_05.png}}\hfill
    \subfloat[sigma: 1.0 ]{\includegraphics[width=0.20\linewidth]{continuous_regression/cont_reg_10.png}}\hfill
    \subfloat[sigma: 2.0 ]{\includegraphics[width=0.20\linewidth]{continuous_regression/cont_reg_20.png}}\hfill
    \subfloat[sigma: 5.0 ]{\includegraphics[width=0.20\linewidth]{continuous_regression/cont_reg_50.png}}\\
    \subfloat[Box plot of effective sample size of weights of each layer]{\includegraphics[width=1.00\linewidth]{continuous_regression/cont_effective_samples.png}}\\
    \caption{Continuous regression using Bayesian Neural Networks}
    \label{continuous_regeression}
\end{figure*}


<p>
We start of our regression analysis my trying fit simple smooth
</p>

<p>
The results can be found in Figure: \ref{continuous_regeression}
</p>
</div>
</div>


<div id="outline-container-sec-5-4-2" class="outline-4">
<h4 id="sec-5-4-2">Analysis of discontinuous regression using Bayesian Neural Networks(numpyro)</h4>
<div class="outline-text-4" id="text-5-4-2">
\begin{figure*}[!htp]
    \subfloat[sigma: 0.2 ]{\includegraphics[width=0.20\linewidth]{discontinuous_regression/discont_reg_02.png}}\hfill
    \subfloat[sigma: 0.5 ]{\includegraphics[width=0.20\linewidth]{discontinuous_regression/discont_reg_05.png}}\hfill
    \subfloat[sigma: 1.0 ]{\includegraphics[width=0.20\linewidth]{discontinuous_regression/discont_reg_10.png}}\hfill
    \subfloat[sigma: 2.0 ]{\includegraphics[width=0.20\linewidth]{discontinuous_regression/discont_reg_20.png}}\hfill
    \subfloat[sigma: 5.0 ]{\includegraphics[width=0.20\linewidth]{discontinuous_regression/discont_reg_50.png}}\\
    \subfloat[Box plot of effective sample size of weights of each layer]{\includegraphics[width=1.00\linewidth]{discontinuous_regression/discont_effective_samples.png}}\\
    \caption{Discontinuous regression using Bayesian Neural Networks}
\end{figure*}

<p>
Previous work on performance Neural Networks on non-smooth functions has been done in \cite{NNnonsmooth}. We extend this work by studying the behaviour of Neural Networks in the bayesian paradigm for non somooth regression. One of the aspects which interests us is the quantified uncertainity produced by the BNN. We also investigated its behaviour with changing prior &sigma; (applied to each weight). 
</p>



<p>
<b>Key Takeaways</b>
</p>

<p>
&sigma; does have a considerable affect on the performance of the BNN. This could be because of excessive constraints on priors with smaller &sigma; when compared to a more relaxed prior for a large &sigma;. This may have lead to a more "regularized" training leading to a "simpler" learned model.
</p>


<p>
NOTE: Regression training of BNN regardless of type of functions require considerable amount of warmup steps when training using MCMC based algorithms like Hamiltonian Monte Carlo(HMC).
</p>
</div>
</div>
</div>
</div>



<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> Conclusion</h2>
</div>

<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7"><span class="section-number-2">7</span> Future Work</h2>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: July, 2019</p>
<p class="author">Author: Sharan Yalburgi, Julyan Arbel</p>
<p class="date">Created: 2019-07-17 Wed 11:04</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.2.2 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
